import pandas as pd
import numpy as np
from itertools import permutations

# ==========================
# KONFIGURACJA – DO EDYCJI
# ==========================

# Ścieżki do plików wejściowych
INPUT_Q3_PATH = r"sciezka/do/pliku_Q3.csv"
INPUT_Q4_PATH = r"sciezka/do/pliku_Q4.csv"

# Typ pliku: "csv" lub "excel"
INPUT_FILE_TYPE = "csv"   # albo "excel"

# Nazwy kolumn w plikach wejściowych
COL_CUSIP      = "cusip"
COL_EAD        = "ead"
COL_RWA        = "rwa"
COL_KG         = "kg"
COL_ATTACH     = "attachment"
COL_DETACH     = "detachment"
COL_W          = "w"
COL_P          = "p"

# Lista faktorów do Shapley (kolejność nie ma znaczenia dla wyniku Shapley)
FACTOR_LIST = [COL_KG, COL_ATTACH, COL_DETACH, COL_W, COL_P]


# ======================================
# FUNKCJA: RISK WEIGHT z SSFA z faktorów
# ======================================

def compute_rw_from_factors(kg, attachment, detachment, w, p):
    """
    TODO: WSTAW TU REALNĄ FORMUŁĘ SSFA DLA RW !!!
    Poniżej jest tylko 'placeholder', żeby skrypt się wykonywał.

    Upewnij się, że jednostki RW są spójne z tym, jak liczysz RWA:
        RWA = RW * EAD
    np. RW jako liczba typu 0.15 (15%) albo 1.25 (125%).
    """

    # ----- PRZYKŁADOWY DUMMY: ZAMIEN NA SWÓJ WZÓR -----
    # Ten dummy zwraca po prostu kg * 12.5 przycięte do 12.5
    # (to NIE jest pełna, poprawna implementacja SSFA!)
    rw = min(kg * 12.5, 12.5)
    return rw


# ==========================
# FUNKCJE POMOCNICZE
# ==========================

def read_input(path: str):
    if INPUT_FILE_TYPE.lower() == "csv":
        return pd.read_csv(path)
    elif INPUT_FILE_TYPE.lower() == "excel":
        return pd.read_excel(path)
    else:
        raise ValueError("INPUT_FILE_TYPE musi być 'csv' albo 'excel'.")


def aggregate_by_cusip(df: pd.DataFrame, period_label: str) -> pd.DataFrame:
    """
    Agregacja po CUSIP:
    - sumujemy EAD i RWA,
    - liczymy EAD-ważone średnie faktorów (kg, attachment, detachment, w, p).
    """
    cols_needed = [
        COL_CUSIP, COL_EAD, COL_RWA,
        COL_KG, COL_ATTACH, COL_DETACH, COL_W, COL_P
    ]
    missing = [c for c in cols_needed if c not in df.columns]
    if missing:
        raise KeyError(f"W pliku brakuje kolumn: {missing}")

    df = df.copy()

    # Liczniki: EAD * faktor
    for col in [COL_KG, COL_ATTACH, COL_DETACH, COL_W, COL_P]:
        df[f"{col}_num"] = df[col] * df[COL_EAD]

    grouped = df.groupby(COL_CUSIP, as_index=False).agg(
        **{
            f"{COL_EAD}_{period_label}": (COL_EAD, "sum"),
            f"{COL_RWA}_{period_label}": (COL_RWA, "sum"),
        }
    )

    num_cols = [f"{COL_KG}_num", f"{COL_ATTACH}_num", f"{COL_DETACH}_num",
                f"{COL_W}_num", f"{COL_P}_num]
    df_num = df.groupby(COL_CUSIP, as_index=False)[num_cols].sum()

    grouped = grouped.merge(df_num, on=COL_CUSIP, how="left")

    # EAD-ważone średnie faktorów
    ead_col = f"{COL_EAD}_{period_label}"

    for col, num_col in zip(
        [COL_KG, COL_ATTACH, COL_DETACH, COL_W, COL_P],
        num_cols
    ):
        grouped[f"{col}_{period_label}"] = grouped[num_col] / grouped[ead_col]
        grouped[f"{col}_{period_label}"] = grouped[f"{col}_{period_label}"].replace(
            [np.inf, -np.inf], np.nan
        )

    grouped.drop(columns=num_cols, inplace=True)

    return grouped


# ======================================
# Shapley dla efektu faktorów per CUSIP
# ======================================

def factor_shapley_effects_row(row) -> pd.Series:
    """
    Shapley values dla efektu poszczególnych faktorów na ΔRWA
    dla pojedynczego CUSIPa.

    Założenie:
      - start: wszystkie faktory w poziomie Q3
      - koniec: wszystkie faktory w poziomie Q4
      - EAD = EAD_Q4 (liczymy efekt na docelowej ekspozycji)

    Dla każdej permutacji faktorów bierzemy marginalny wkład danego faktora
    (zmiana RW przy przejściu Q3->Q4 dla tego faktora) i potem uśredniamy
    po wszystkich permutacjach.

    Wynik: Series z kolumnami "<factor>_effect" w jednostkach RWA.
    """
    ead4 = row[f"{COL_EAD}_Q4"]

    # Jeśli EAD w Q4 = 0 lub NaN, nie ma efektu faktorów (RWA=0)
    if pd.isna(ead4) or ead4 == 0:
        return pd.Series({f"{f}_effect": 0.0 for f in FACTOR_LIST})

    # Faktor Q3 i Q4 w słownikach
    factors_q3 = {
        COL_KG:      row.get(f"{COL_KG}_Q3"),
        COL_ATTACH:  row.get(f"{COL_ATTACH}_Q3"),
        COL_DETACH:  row.get(f"{COL_DETACH}_Q3"),
        COL_W:       row.get(f"{COL_W}_Q3"),
        COL_P:       row.get(f"{COL_P}_Q3"),
    }
    factors_q4 = {
        COL_KG:      row.get(f"{COL_KG}_Q4"),
        COL_ATTACH:  row.get(f"{COL_ATTACH}_Q4"),
        COL_DETACH:  row.get(f"{COL_DETACH}_Q4"),
        COL_W:       row.get(f"{COL_W}_Q4"),
        COL_P:       row.get(f"{COL_P}_Q4"),
    }

    # Jeśli czegoś brakuje (NaN), traktujemy jako 0 – możesz to dostosować
    for f in FACTOR_LIST:
        if pd.isna(factors_q3[f]):
            factors_q3[f] = 0.0
        if pd.isna(factors_q4[f]):
            factors_q4[f] = 0.0

    # Inicjalizacja sum shapley'owskich
    shapley_sums = {f: 0.0 for f in FACTOR_LIST}

    # Wszystkie permutacje faktorów
    perms = list(permutations(FACTOR_LIST))
    n_perm = len(perms)

    for perm in perms:
        # Start: wszystkie faktory w poziomie Q3
        curr_factors = factors_q3.copy()

        rw_prev = compute_rw_from_factors(
            kg=curr_factors[COL_KG],
            attachment=curr_factors[COL_ATTACH],
            detachment=curr_factors[COL_DETACH],
            w=curr_factors[COL_W],
            p=curr_factors[COL_P],
        )

        for f in perm:
            # podmieniamy tylko jeden faktor na wartość Q4
            curr_factors[f] = factors_q4[f]

            rw_new = compute_rw_from_factors(
                kg=curr_factors[COL_KG],
                attachment=curr_factors[COL_ATTACH],
                detachment=curr_factors[COL_DETACH],
                w=curr_factors[COL_W],
                p=curr_factors[COL_P],
            )

            delta_rw = rw_new - rw_prev

            # marginalny wkład tego faktora w tej permutacji
            shapley_sums[f] += delta_rw

            rw_prev = rw_new

    # Uśrednienie po wszystkich permutacjach i przeskalowanie przez EAD_Q4
    effects = {
        f"{f}_effect": (shapley_sums[f] / n_perm) * ead4
        for f in FACTOR_LIST
    }

    return pd.Series(effects)


# ==========================
# GŁÓWNA LOGIKA
# ==========================

def main():
    # 1. Wczytanie danych
    df_q3_raw = read_input(INPUT_Q3_PATH)
    df_q4_raw = read_input(INPUT_Q4_PATH)

    # 2. Agregacja po CUSIP
    df_q3 = aggregate_by_cusip(df_q3_raw, "Q3")
    df_q4 = aggregate_by_cusip(df_q4_raw, "Q4")

    # 3. Połączenie Q3 i Q4 po CUSIP
    df = df_q3.merge(df_q4, on=COL_CUSIP, how="outer", indicator=True)

    # 4. Podstawowe sumy / total RWA
    total_rwa_q3 = df[f"{COL_RWA}_Q3"].sum(skipna=True)
    total_rwa_q4 = df[f"{COL_RWA}_Q4"].sum(skipna=True)
    delta_rwa_total = total_rwa_q4 - total_rwa_q3

    # 5. Maski: nowe, znikniete, wspólne
    mask_new     = df["_merge"] == "right_only"   # tylko w Q4
    mask_gone    = df["_merge"] == "left_only"    # tylko w Q3
    mask_common  = df["_merge"] == "both"         # wspólne

    # 6. Efekt nowych papierów (RWA nowych CUSIPów)
    new_papers_effect = df.loc[mask_new, f"{COL_RWA}_Q4"].sum(skipna=True)

    # 7. Efekt zniknięcia papierów (sprzedaż / full paydown)
    gone_papers_effect = - df.loc[mask_gone, f"{COL_RWA}_Q3"].sum(skipna=True)

    # 8. Analiza wspólnych CUSIPów (zmiana EAD i faktorów)
    common = df.loc[mask_common].copy()

    # RW z danych (fallback / sanity check)
    common["RW_Q3_from_data"] = common[f"{COL_RWA}_Q3"] / common[f"{COL_EAD}_Q3"]
    common["RW_Q4_from_data"] = common[f"{COL_RWA}_Q4"] / common[f"{COL_EAD}_Q4"]
    common["RW_Q3_from_data"] = common["RW_Q3_from_data"].replace(
        [np.inf, -np.inf], 0
    ).fillna(0)
    common["RW_Q4_from_data"] = common["RW_Q4_from_data"].replace(
        [np.inf, -np.inf], 0
    ).fillna(0)

    # 8a. Efekt zmiany ekspozycji na wspólnych papierach:
    #      Exposure effect = RW_Q3 * (EAD_Q4 - EAD_Q3)
    common["exposure_effect"] = common["RW_Q3_from_data"] * (
        common[f"{COL_EAD}_Q4"] - common[f"{COL_EAD}_Q3"]
    )
    exposure_effect_existing = common["exposure_effect"].sum(skipna=True)

    # 8b. Efekt faktorów: Shapley per faktor
    factor_effects_df = common.apply(factor_shapley_effects_row, axis=1)
    for col in factor_effects_df.columns:
        common[col] = factor_effects_df[col]

    factor_effect_sums = {
        col: common[col].sum(skipna=True) for col in factor_effects_df.columns
    }
    total_factor_effect = sum(factor_effect_sums.values())

    # 9. Złożenie dekompozycji
    total_explained = (
        new_papers_effect
        + gone_papers_effect
        + exposure_effect_existing
        + total_factor_effect
    )

    unexplained = delta_rwa_total - total_explained

    # ==========================
    # OUTPUT – PODSUMOWANIE
    # ==========================

    print("===== PODSUMOWANIE RWA ATTRIBUTION (SSFA, Shapley) =====")
    print(f"Total RWA Q3: {total_rwa_q3:,.2f}")
    print(f"Total RWA Q4: {total_rwa_q4:,.2f}")
    print(f"ΔRWA (Q4 - Q3): {delta_rwa_total:,.2f}")
    print()

    print("Kontrybucje (signed):")
    print(f"  + Nowe papiery (tylko Q4):         {new_papers_effect:,.2f}")
    print(f"  + Zniknięte papiery (sell/payoff): {gone_papers_effect:,.2f}")
    print(f"  + Zmiana EAD na wspólnych:         {exposure_effect_existing:,.2f}")
    print(f"  + Efekt faktorów ogółem (Shapley): {total_factor_effect:,.2f}")
    print()

    print("Rozbicie efektu faktorów (Shapley):")
    for k, v in factor_effect_sums.items():
        factor_name = k.replace("_effect", "")
        print(f"    - {factor_name}: {v:,.2f}")

    print()
    print(f"Suma wyjaśniona (powinna ≈ ΔRWA): {total_explained:,.2f}")
    print(f"Niewyjaśnione (rounding / model): {unexplained:,.2f}")


if __name__ == "__main__":
    main()
