import pandas as pd
import numpy as np
from itertools import permutations

# ==========================
# KONFIGURACJA – DO EDYCJI
# ==========================

# Ścieżki do plików wejściowych
INPUT_Q3_PATH = r"sciezka/do/pliku_Q3.csv"
INPUT_Q4_PATH = r"sciezka/do/pliku_Q4.csv"

# Typ pliku: "csv" lub "excel"
INPUT_FILE_TYPE = "csv"   # albo "excel"

# Nazwy kolumn w plikach wejściowych
COL_CUSIP        = "cusip"
COL_EAD          = "ead"
COL_RWA          = "rwa"
COL_KG           = "kg"
COL_ATTACH       = "attachment"
COL_DETACH       = "detachment"
COL_W            = "w"
COL_P            = "p"
COL_ASSET_CLASS  = "asset_class"   # asset class

# Lista faktorów
FACTOR_LIST = [COL_KG, COL_ATTACH, COL_DETACH, COL_W, COL_P]


# ======================================
# FUNKCJA: RISK WEIGHT z SSFA z faktorów
# ======================================

def compute_rw_from_factors(kg, attachment, detachment, w, p):
    """
    TODO: WSTAW TU REALNĄ FORMUŁĘ SSFA DLA RW !!!
    Poniżej placeholder, żeby skrypt się wykonywał.
    """
    rw = min(kg * 12.5, 12.5)  # DUMMY
    return rw


# ==========================
# FUNKCJE POMOCNICZE
# ==========================

def read_input(path: str):
    if INPUT_FILE_TYPE.lower() == "csv":
        return pd.read_csv(path)
    elif INPUT_FILE_TYPE.lower() == "excel":
        return pd.read_excel(path)
    else:
        raise ValueError("INPUT_FILE_TYPE musi być 'csv' albo 'excel'.")


def aggregate_by_cusip(df: pd.DataFrame, period_label: str) -> pd.DataFrame:
    """
    Agregacja po CUSIP:
    - sumujemy EAD i RWA,
    - bierzemy asset_class = 'first' (stałe per CUSIP),
    - liczymy EAD-ważone średnie faktorów (kg, attachment, detachment, w, p).
    """
    cols_needed = [
        COL_CUSIP, COL_EAD, COL_RWA,
        COL_KG, COL_ATTACH, COL_DETACH, COL_W, COL_P,
        COL_ASSET_CLASS
    ]
    missing = [c for c in cols_needed if c not in df.columns]
    if missing:
        raise KeyError(f"W pliku brakuje kolumn: {missing}")

    df = df.copy()

    # Liczniki: EAD * faktor
    for col in FACTOR_LIST:
        df[f"{col}_num"] = df[col] * df[COL_EAD]

    agg_dict = {
        COL_EAD: "sum",
        COL_RWA: "sum",
        COL_ASSET_CLASS: "first",
    }
    for col in FACTOR_LIST:
        agg_dict[f"{col}_num"] = "sum"

    grouped = df.groupby(COL_CUSIP, as_index=False).agg(agg_dict)

    grouped.rename(columns={
        COL_EAD:         f"{COL_EAD}_{period_label}",
        COL_RWA:         f"{COL_RWA}_{period_label}",
        COL_ASSET_CLASS: f"{COL_ASSET_CLASS}_{period_label}",
    }, inplace=True)

    ead_col = f"{COL_EAD}_{period_label}"

    # EAD-ważone średnie faktorów
    for col in FACTOR_LIST:
        num_col = f"{col}_num"
        grouped[f"{col}_{period_label}"] = grouped[num_col] / grouped[ead_col]
        grouped[f"{col}_{period_label}"] = grouped[f"{col}_{period_label}"].replace(
            [np.inf, -np.inf], np.nan
        )

    num_cols = [f"{col}_num" for col in FACTOR_LIST]
    grouped.drop(columns=num_cols, inplace=True)

    return grouped


# ======================================
# Shapley dla efektu faktorów per CUSIP
# ======================================

def factor_shapley_effects_row(row) -> pd.Series:
    ead4 = row[f"{COL_EAD}_Q4"]

    if pd.isna(ead4) or ead4 == 0:
        return pd.Series({f"{f}_shapley_effect": 0.0 for f in FACTOR_LIST})

    factors_q3 = {
        COL_KG:      row.get(f"{COL_KG}_Q3"),
        COL_ATTACH:  row.get(f"{COL_ATTACH}_Q3"),
        COL_DETACH:  row.get(f"{COL_DETACH}_Q3"),
        COL_W:       row.get(f"{COL_W}_Q3"),
        COL_P:       row.get(f"{COL_P}_Q3"),
    }
    factors_q4 = {
        COL_KG:      row.get(f"{COL_KG}_Q4"),
        COL_ATTACH:  row.get(f"{COL_ATTACH}_Q4"),
        COL_DETACH:  row.get(f"{COL_DETACH}_Q4"),
        COL_W:       row.get(f"{COL_W}_Q4"),
        COL_P:       row.get(f"{COL_P}_Q4"),
    }

    for f in FACTOR_LIST:
        if pd.isna(factors_q3[f]):
            factors_q3[f] = 0.0
        if pd.isna(factors_q4[f]):
            factors_q4[f] = 0.0

    shapley_sums = {f: 0.0 for f in FACTOR_LIST}
    perms = list(permutations(FACTOR_LIST))
    n_perm = len(perms)

    for perm in perms:
        curr_factors = factors_q3.copy()

        rw_prev = compute_rw_from_factors(
            kg=curr_factors[COL_KG],
            attachment=curr_factors[COL_ATTACH],
            detachment=curr_factors[COL_DETACH],
            w=curr_factors[COL_W],
            p=curr_factors[COL_P],
        )

        for f in perm:
            curr_factors[f] = factors_q4[f]

            rw_new = compute_rw_from_factors(
                kg=curr_factors[COL_KG],
                attachment=curr_factors[COL_ATTACH],
                detachment=curr_factors[COL_DETACH],
                w=curr_factors[COL_W],
                p=curr_factors[COL_P],
            )

            delta_rw = rw_new - rw_prev
            shapley_sums[f] += delta_rw

            rw_prev = rw_new

    effects = {
        f"{f}_shapley_effect": (shapley_sums[f] / n_perm) * ead4
        for f in FACTOR_LIST
    }

    return pd.Series(effects)


# ======================================
# PURE (counterfactual) efekt faktorów
# ======================================

def factor_pure_effects_row(row) -> pd.Series:
    ead4 = row[f"{COL_EAD}_Q4"]

    if pd.isna(ead4) or ead4 == 0:
        return pd.Series({f"{f}_pure_effect": 0.0 for f in FACTOR_LIST})

    factors_q3 = {
        COL_KG:      row.get(f"{COL_KG}_Q3"),
        COL_ATTACH:  row.get(f"{COL_ATTACH}_Q3"),
        COL_DETACH:  row.get(f"{COL_DETACH}_Q3"),
        COL_W:       row.get(f"{COL_W}_Q3"),
        COL_P:       row.get(f"{COL_P}_Q3"),
    }
    factors_q4 = {
        COL_KG:      row.get(f"{COL_KG}_Q4"),
        COL_ATTACH:  row.get(f"{COL_ATTACH}_Q4"),
        COL_DETACH:  row.get(f"{COL_DETACH}_Q4"),
        COL_W:       row.get(f"{COL_W}_Q4"),
        COL_P:       row.get(f"{COL_P}_Q4"),
    }

    for f in FACTOR_LIST:
        if pd.isna(factors_q3[f]):
            factors_q3[f] = 0.0
        if pd.isna(factors_q4[f]):
            factors_q4[f] = 0.0

    rw_q3 = compute_rw_from_factors(
        kg=factors_q3[COL_KG],
        attachment=factors_q3[COL_ATTACH],
        detachment=factors_q3[COL_DETACH],
        w=factors_q3[COL_W],
        p=factors_q3[COL_P],
    )

    effects = {}

    for f in FACTOR_LIST:
        curr = factors_q3.copy()
        curr[f] = factors_q4[f]

        rw_cf = compute_rw_from_factors(
            kg=curr[COL_KG],
            attachment=curr[COL_ATTACH],
            detachment=curr[COL_DETACH],
            w=curr[COL_W],
            p=curr[COL_P],
        )

        delta_rw = rw_cf - rw_q3
        effects[f"{f}_pure_effect"] = delta_rw * ead4

    return pd.Series(effects)


# ==========================
# GŁÓWNA LOGIKA
# ==========================

def main():
    # 1. Wczytanie danych
    df_q3_raw = read_input(INPUT_Q3_PATH)
    df_q4_raw = read_input(INPUT_Q4_PATH)

    # 2. Agregacja po CUSIP
    df_q3 = aggregate_by_cusip(df_q3_raw, "Q3")
    df_q4 = aggregate_by_cusip(df_q4_raw, "Q4")

    # 3. Połączenie Q3 i Q4 po CUSIP
    df = df_q3.merge(df_q4, on=COL_CUSIP, how="outer", indicator=True)

    # asset_class (stałe per CUSIP)
    df[COL_ASSET_CLASS] = df[f"{COL_ASSET_CLASS}_Q3"].combine_first(
        df[f"{COL_ASSET_CLASS}_Q4"]
    )

    # Status: new / gone / common
    df["status"] = df["_merge"].map({
        "left_only":  "gone",
        "right_only": "new",
        "both":       "common"
    })

    # 4. Total RWA
    total_rwa_q3 = df[f"{COL_RWA}_Q3"].sum(skipna=True)
    total_rwa_q4 = df[f"{COL_RWA}_Q4"].sum(skipna=True)
    delta_rwa_total = total_rwa_q4 - total_rwa_q3

    # 5. Maski
    mask_new     = df["_merge"] == "right_only"
    mask_gone    = df["_merge"] == "left_only"
    mask_common  = df["_merge"] == "both"

    # RW z danych (dla wszystkich – new/gone będą mieć NaN/0)
    df["RW_Q3_from_data"] = df[f"{COL_RWA}_Q3"] / df[f"{COL_EAD}_Q3"]
    df["RW_Q4_from_data"] = df[f"{COL_RWA}_Q4"] / df[f"{COL_EAD}_Q4"]
    df["RW_Q3_from_data"] = df["RW_Q3_from_data"].replace(
        [np.inf, -np.inf], 0
    ).fillna(0)
    df["RW_Q4_from_data"] = df["RW_Q4_from_data"].replace(
        [np.inf, -np.inf], 0
    ).fillna(0)

    # 6. Efekt nowych / znikniętych
    new_papers_effect = df.loc[mask_new, f"{COL_RWA}_Q4"].sum(skipna=True)
    gone_papers_effect = - df.loc[mask_gone, f"{COL_RWA}_Q3"].sum(skipna=True)

    # 7. Wspólne CUSIPy
    common = df.loc[mask_common].copy()

    # Exposure effect (tylko common)
    common["exposure_effect"] = common["RW_Q3_from_data"] * (
        common[f"{COL_EAD}_Q4"] - common[f"{COL_EAD}_Q3"]
    )
    exposure_effect_existing = common["exposure_effect"].sum(skipna=True)

    # Shapley (tylko common)
    shapley_df = common.apply(factor_shapley_effects_row, axis=1)
    for col in shapley_df.columns:
        common[col] = shapley_df[col]

    shapley_sums = {
        col: common[col].sum(skipna=True) for col in shapley_df.columns
    }
    total_shapley_effect = sum(shapley_sums.values())

    # Pure (tylko common)
    pure_df = common.apply(factor_pure_effects_row, axis=1)
    for col in pure_df.columns:
        common[col] = pure_df[col]

    pure_sums = {
        col: common[col].sum(skipna=True) for col in pure_df.columns
    }
    total_pure_effect = sum(pure_sums.values())

    # 8. ΔEAD, ΔRWA, ΔRW (na df – dla wszystkich statusów)
    df["delta_ead"] = df[f"{COL_EAD}_Q4"] - df[f"{COL_EAD}_Q3"]
    df["delta_rwa"] = df[f"{COL_RWA}_Q4"] - df[f"{COL_RWA}_Q3"]
    df["delta_rw"]  = df["RW_Q4_from_data"] - df["RW_Q3_from_data"]

    # 9. Złożenie dekompozycji (global)
    total_explained = (
        new_papers_effect
        + gone_papers_effect
        + exposure_effect_existing
        + total_shapley_effect
    )
    unexplained = delta_rwa_total - total_explained

    # ==========================
    # OUTPUT – GLOBAL (print)
    # ==========================

    print("===== RWA ATTRIBUTION (SSFA) – CAŁY PORTFEL =====")
    print(f"Total RWA Q3: {total_rwa_q3:,.2f}")
    print(f"Total RWA Q4: {total_rwa_q4:,.2f}")
    print(f"ΔRWA (Q4 - Q3): {delta_rwa_total:,.2f}")
    print()
    print("Kontrybucje:")
    print(f"  New:       {new_papers_effect:,.2f}")
    print(f"  Gone:      {gone_papers_effect:,.2f}")
    print(f"  Exposure:  {exposure_effect_existing:,.2f}")
    print(f"  Factors:   {total_shapley_effect:,.2f}")
    print(f"  Explained: {total_explained:,.2f}")
    print(f"  Unexpl.:   {unexplained:,.2f}")

    # ==========================
    # BY ASSET CLASS (jak wcześniej)
    # ==========================

    asset_classes = df[COL_ASSET_CLASS].dropna().unique()
    asset_classes = sorted(asset_classes)

    results_by_class = []

    for cls in asset_classes:
        sub_all = df[df[COL_ASSET_CLASS] == cls]
        total_rwa_q3_cls = sub_all[f"{COL_RWA}_Q3"].sum(skipna=True)
        total_rwa_q4_cls = sub_all[f"{COL_RWA}_Q4"].sum(skipna=True)
        delta_rwa_cls = total_rwa_q4_cls - total_rwa_q3_cls

        new_eff_cls = sub_all.loc[sub_all["status"] == "new", f"{COL_RWA}_Q4"].sum(skipna=True)
        gone_eff_cls = - sub_all.loc[sub_all["status"] == "gone", f"{COL_RWA}_Q3"].sum(skipna=True)

        common_cls = common[common[COL_ASSET_CLASS] == cls]
        exposure_eff_cls = common_cls["exposure_effect"].sum(skipna=True)

        shapley_cls_sums = {
            f"{f}_shapley_effect": common_cls[f"{f}_shapley_effect"].sum(skipna=True)
            for f in FACTOR_LIST
        }
        shapley_cls_total = sum(shapley_cls_sums.values())

        pure_cls_sums = {
            f"{f}_pure_effect": common_cls[f"{f}_pure_effect"].sum(skipna=True)
            for f in FACTOR_LIST
        }
        pure_cls_total = sum(pure_cls_sums.values())

        explained_cls = (
            new_eff_cls
            + gone_eff_cls
            + exposure_eff_cls
            + shapley_cls_total
        )
        unexplained_cls = delta_rwa_cls - explained_cls

        results_by_class.append({
            "asset_class": cls,
            "total_rwa_q3": total_rwa_q3_cls,
            "total_rwa_q4": total_rwa_q4_cls,
            "delta_rwa": delta_rwa_cls,
            "new": new_eff_cls,
            "gone": gone_eff_cls,
            "exposure": exposure_eff_cls,
            "factor_shapley": shapley_cls_total,
            "factor_pure": pure_cls_total,
            "explained": explained_cls,
            "unexplained": unexplained_cls,
            **{f"{f}_shapley": shapley_cls_sums[f"{f}_shapley_effect"] for f in FACTOR_LIST},
            **{f"{f}_pure":    pure_cls_sums[f"{f}_pure_effect"]      for f in FACTOR_LIST},
        })

    df_results_by_class = pd.DataFrame(results_by_class)

    # ==========================
    # PER-CUSIP EXPORT
    # ==========================

    # Najpierw wstrzykujemy efekty z 'common' z powrotem do df (dla common rows)
    effects_cols = (
        ["exposure_effect"] +
        [f"{f}_shapley_effect" for f in FACTOR_LIST] +
        [f"{f}_pure_effect" for f in FACTOR_LIST]
    )
    common_effects = common[[COL_CUSIP] + effects_cols].copy()

    df = df.merge(common_effects, on=COL_CUSIP, how="left", suffixes=("", "_from_common"))

    # Kolumny do outputu per CUSIP
    per_cusip_cols = [
        COL_CUSIP,
        COL_ASSET_CLASS,
        "status",
        f"{COL_EAD}_Q3", f"{COL_EAD}_Q4", "delta_ead",
        f"{COL_RWA}_Q3", f"{COL_RWA}_Q4", "delta_rwa",
        "RW_Q3_from_data", "RW_Q4_from_data", "delta_rw",
        "exposure_effect",
    ] + [f"{f}_shapley_effect" for f in FACTOR_LIST] \
      + [f"{f}_pure_effect" for f in FACTOR_LIST]

    # A) tylko common (status == 'common')
    df_cusip_common = df[df["status"] == "common"][per_cusip_cols].copy()

    # B) wszystkie cusipy
    df_cusip_all = df[per_cusip_cols].copy()

    # ==========================
    # ZAPIS DO EXCELA
    # ==========================

    # 1) by asset class + global summary (opcjonalnie)
    with pd.ExcelWriter("rwa_attr_by_asset_class.xlsx") as writer:
        df_results_by_class.to_excel(writer, sheet_name="by_asset_class", index=False)

    # 2) per-CUSIP tylko common
    with pd.ExcelWriter("rwa_attr_per_cusip_common.xlsx") as writer:
        df_cusip_common.to_excel(writer, sheet_name="common_only", index=False)

    # 3) per-CUSIP wszystkie (new/gone/common)
    with pd.ExcelWriter("rwa_attr_per_cusip_all.xlsx") as writer:
        df_cusip_all.to_excel(writer, sheet_name="all_cusips", index=False)

    print("\nPliki zapisane:")
    print(" - rwa_attr_by_asset_class.xlsx")
    print(" - rwa_attr_per_cusip_common.xlsx")
    print(" - rwa_attr_per_cusip_all.xlsx")


if __name__ == "__main__":
    main()
