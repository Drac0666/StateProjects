import os
import requests

def download_ecb_assets(url, folder_path, file_name):
    # Ensure the folder exists
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)

    # Combine the folder path and file name to get the full file path
    file_path = os.path.join(folder_path, file_name)

    # Make the request to download the ECB assets data
    response = requests.get(url)

    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Write the content to the file
        with open(file_path, 'wb') as file:
            file.write(response.content)
        print(f"File downloaded successfully to {file_path}")
    else:
        print(f"Failed to download file. Status code: {response.status_code}")

# Example usage:
ecb_assets_url = "https://www.ecb.europa.eu/paym/coll/assets/html/dla/ea_MID/ea_csv_231218.csv"
download_folder = "path/to/your/folder"
ecb_assets_file_name = "ecb_assets.csv"

download_ecb_assets(ecb_assets_url, download_folder, ecb_assets_file_name)












import pandas as pd

# Sample DataFrame
data = {'Sub-Asset Class': ['Auto ABS', 'Auto ABS', 'Corporate Bonds', 'Corporate Bonds', 'Corporate Bonds'],
        'Par Value': [1000, 4000, 500, 300, 200]}

df = pd.DataFrame(data, index=pd.to_datetime(['2023-10-31', '2023-11-01', '2023-11-02', '2023-11-03', '2023-11-04']))

# Define the specific date for the month you want to filter
specific_date = '2023-10-31'

# Create a Grouper object to group by month
grouper = pd.Grouper(freq='M', key='index')

# Group the DataFrame by month
grouped = df.groupby([grouper, 'Sub-Asset Class']).agg({'Par Value': ['count', 'sum']})

# Extract the year and month from the specific date
year, month = specific_date.split('-')[0], specific_date.split('-')[1]

# Get the group for the specific month
specific_month_group = grouped.loc[pd.to_datetime(f'{year}-{month}-01')]

# Create the desired output text
output_text = []
for (date, sub_asset_class), (count, total_par_value) in specific_month_group.iterrows():
    output_text.append(f'{count} {sub_asset_class} with Par Value of {total_par_value}')

# Join the output texts
result = ', '.join(output_text)

print(f"Summary for {year}-{month}: {result}")



============================================

# Filter the DataFrame for the specific date
filtered_df = df[df.index == specific_date]

# Group by "Sub-Asset Class" and calculate count and sum
grouped = filtered_df.groupby('Sub-Asset Class').agg({'Par Value': ['count', 'sum']})

# Create the desired output text
output_text = []
for sub_asset_class, (count, total_par_value) in grouped.iterrows():
    output_text.append(f'{count} {sub_asset_class} with Par Value of {total_par_value}')

# Join the output texts
result = ', '.join(output_text)

print(f"Summary for {specific_date}: {result}")




import pandas as pd

# Sample DataFrame
data = {'Sub-Asset Class': ['Auto ABS', 'Auto ABS', 'Corporate Bonds', 'Corporate Bonds', 'Corporate Bonds'],
        'Par Value': [1000, 4000, 500, 300, 200]}

df = pd.DataFrame(data, index=pd.to_datetime(['2023-11-01', '2023-11-02', '2023-11-03', '2023-11-04', '2023-11-05']))

# Group by "Sub-Asset Class" and calculate count and sum
grouped = df.groupby('Sub-Asset Class').agg({'Par Value': ['count', 'sum']})

# Create the desired output text
output_text = []
for sub_asset_class, (count, total_par_value) in grouped.iterrows():
    output_text.append(f'{count} {sub_asset_class} with Par Value of {total_par_value}')

# Join the output texts
result = ', '.join(output_text)

print(result)

=INDIRECT("'" & A2 & "'!A1")

from datetime import datetime, timedelta
import pandas as pd

def generate_month_range(start_month, end_month):
    # Parse the input months as datetime objects
    start_date = pd.to_datetime(start_month, format='%b %y')
    end_date = pd.to_datetime(end_month, format='%b %y')

    # Initialize the result list
    month_range = [start_date.strftime('%b %y')]

    # Generate months in the range
    while start_date < end_date:
        # Add one month to the current date
        start_date += pd.DateOffset(months=1)
        month_range.append(start_date.strftime('%b %y'))

    return month_range

# Example usage
start_month = "Jan 22"
end_month = "Apr 22"
result = generate_month_range(start_month, end_month)
print(result)




import pandas as pd

# Sample DataFrame with missing columns
data = {'Jan 22': [1, 2, 3],
        'Apr 22': [4, 5, 6],
        'Apr 23': [7, 8, 9]}

df = pd.DataFrame(data)

# Function to fill gaps and replicate data with inferred reporting frequency
def fill_and_replicate(df):
    columns = df.columns
    new_columns = [columns[0]]
    reporting_frequency = None

    # Infer reporting frequency based on month differences
    for i in range(1, len(columns)):
        current_month = pd.to_datetime(columns[i], format='%b %y')
        prev_month = pd.to_datetime(columns[i - 1], format='%b %y')
        month_diff = (current_month.year - prev_month.year) * 12 + (current_month.month - prev_month.month)

        if reporting_frequency is None:
            reporting_frequency = month_diff
        elif month_diff != reporting_frequency:
            reporting_frequency = 1  # Use 1 as a default for unknown reporting frequencies
            break

    for i in range(1, len(columns)):
        current_month = pd.to_datetime(columns[i], format='%b %y')
        prev_month = pd.to_datetime(columns[i - 1], format='%b %y')
        month_diff = (current_month.year - prev_month.year) * 12 + (current_month.month - prev_month.month)

        for j in range(1, month_diff // reporting_frequency):
            new_date = prev_month + pd.DateOffset(months=j * reporting_frequency)
            new_columns.append(new_date.strftime('%b %y'))

        new_columns.append(columns[i])

    df = df[new_columns]

    return df

# Call the function to fill and replicate the DataFrame
df = fill_and replicate(df)

# Display the modified DataFrame
print(df)
KAKAAK

import pandas as pd
import numpy as np

# Sample DataFrame with missing columns
data = {'Jan 22': [1, 2, 3],
        'Apr 22': [4, 5, 6],
        'Apr 23': [7, 8, 9]}

df = pd.DataFrame(data)

# Function to fill gaps and replicate data with inferred reporting frequency
def fill_and_replicate(df):
    columns = df.columns
    new_columns = [columns[0]]
    reporting_frequency = None

    # Infer reporting frequency based on time differences
    for i in range(1, len(columns)):
        current_date = pd.to_datetime(columns[i], format='%b %y')
        prev_date = pd.to_datetime(columns[i-1], format='%b %y')
        diff = (current_date - prev_date).days
        if reporting_frequency is None:
            reporting_frequency = diff
        elif diff != reporting_frequency:
            reporting_frequency = 1  # Use 1 as a default for unknown reporting frequencies
            break

    for i in range(1, len(columns)):
        current_date = pd.to_datetime(columns[i], format='%b %y')
        prev_date = pd.to_datetime(columns[i-1], format='%b %y')
        diff = (current_date - prev_date).days

        for j in range(1, diff // reporting_frequency):
            new_date = prev_date + pd.DateOffset(days=j * reporting_frequency)
            new_columns.append(new_date.strftime('%b %y'))

        new_columns.append(columns[i])

    df = df[new_columns]

    return df

# Call the function to fill and replicate the DataFrame
df = fill_and_replicate(df)

# Display the modified DataFrame
print(df)

==================================import pandas as pd

def calculate_relative_change(series):
    pct_change = series.pct_change() * 100  # Convert to percentage
    pct_change[0] = 0  # Set the first value to 0
    pct_change = np.where(np.isinf(pct_change), 0, pct_change)  # Replace inf with 0
    return pct_change

def calculate_logrelative_change(series):
    log_returns = np.log(series / series.shift(1))  # Calculate log-return
    log_returns.fillna(0, inplace=True)  # Replace NaN (for the first row) with 0
    return log_returns


# Sample DataFrame
data = {'A': [10, 15, 20, 25],
        'B': [5, 10, 30, 40],
        'C': [50, 70, 90, 110]}
df = pd.DataFrame(data)

# Function to calculate relative change
def calculate_relative_change(series):
    return series.pct_change() * 100  # Convert to percentage

# Categorize changes and assign letters
def categorize_changes(change, column_name):
    if column_name in ExceptionVariableList:
        change *= -1
    if change >= 80:
        return 'E'
    elif change >= 60:
        return 'D'
    elif change >= 40:
        return 'C'
    elif change >= 20:
        return 'B'
    elif change >= 0:
        return 'A'
    elif change >= -20:
        return 'F'
    elif change >= -40:
        return 'G'
    elif change >= -60:
        return 'H'
    elif change >= -80:
        return 'I'
    else:
        return 'J'

# List of columns to be treated as exceptions
ExceptionVariableList = ['A']

# Iterate through columns and add "_Diff" and "_Category" columns
for column in df.columns:
    new_column_name = f'{column}_Diff'
    df[new_column_name] = calculate_relative_change(df[column])
    
    category_column_name = f'{column}_Category'
    df[category_column_name] = df.apply(lambda row: categorize_changes(row[new_column_name], column), axis=1)

# Display the modified DataFrame
print(df)


# Iterate through columns and add "_Diff" and "_Category" columns
for column in df.columns:
    new_column_name = f'{column}_Diff'
    df[new_column_name] = calculate_relative_change(df[column])
    
    category_column_name = f'{column}_Category'
    df[category_column_name] = df[new_column_name].apply(categorize_changes)

# Display the modified DataFrame
print(df)


# Iterate through columns and add "_Diff" columns
for column in df.columns:
    new_column_name = f'{column}_Diff'
    df[new_column_name] = calculate_relative_change(df[column])

and not pd.isna(df.loc[date_index, column_to_calculate])]

import pandas as pd

# Sample dictionary of dataframes with unique constant values in column 'A' and date index
dataframes = {
    'df1': pd.DataFrame({'A': ['Test'], 'Value1': [10, 20, 30], 'Value2': [40, 50, 60], 'Value3': [70, 80, 90]}, index=['01 22', '02 22', '03 22']),
    'df2': pd.DataFrame({'A': ['Kurwa'], 'Value1': [40, 50, 60], 'Value2': [10, 20, 30], 'Value3': [70, 80, 90]}, index=['01 22', '02 22', '03 22']),
    'df3': pd.DataFrame({'A': ['Test'], 'Value1': [70, 80], 'Value2': [40, 50], 'Value3': [10, 20]}, index=['01 22', '02 22']),
    'df4': pd.DataFrame({'A': ['Kurwa'], 'Value1': [100, 110, 120], 'Value2': [30, 40, 50], 'Value3': [20, 30, 40]}, index=['01 22', '02 22', '03 22']),
}

# Create a dictionary to categorize dataframes
categorized_dataframes = {}

# Specify the column to use for categorization
categorize_by_column = 'A'

# Specify the category for which you want to calculate the means
category_to_calculate = 'Test'

# Specify the column for which you want to calculate the means
column_to_calculate = 'Value2'

# Initialize a dictionary to store means for each index
means_by_index = {}

# Iterate through dataframes and categorize
for df_name, df in dataframes.items():
    category = df[categorize_by_column].iloc[0]
    category_lower = category.lower()  # Convert category to lowercase
    if category_lower not in categorized_dataframes:
        categorized_dataframes[category_lower] = []
    categorized_dataframes[category_lower].append(df)

# Calculate the mean for the specified column at each available index within the specified category
if category_to_calculate in categorized_dataframes:
    category_dataframes = categorized_dataframes[category_to_calculate]
    
    # Get the unique date indices from the dataframes
    date_indices = set()
    for df in category_dataframes:
        date_indices.update(df.index)
    
    # Calculate means for the specified column at each unique date index
    for date_index in date_indices:
        values = [df.loc[date_index, column_to_calculate] for df in category_dataframes if date_index in df.index]
        mean_value = sum(values) / len(values)
        means_by_index[date_index] = mean_value

# Print the list of means for the specified category and column
for date_index, mean in means_by_index.items():
    print(f"Mean value for category '{category_to_calculate}', column '{column_to_calculate}' at date index '{date_index}': {mean}")


import pandas as pd

# Sample dictionary of dataframes with unique constant values in column 'A' and date index
dataframes = {
    'df1': pd.DataFrame({'A': ['Test'], 'Value': [10, 20, 30,50]}, index=['01 22', '02 22', '03 22','04 22']),
    'df2': pd.DataFrame({'A': ['Kurwa'], 'Value': [40, 50, 60]}, index=['01 22', '02 22', '03 22']),
    'df3': pd.DataFrame({'A': ['Test'], 'Value': [70, 80]}, index=['01 22', '02 22']),
    'df4': pd.DataFrame({'A': ['Kurwa'], 'Value': [100, 110, 120]}, index=['01 22', '02 22', '03 22']),
}

# Create a dictionary to categorize dataframes
categorized_dataframes = {}

# Specify the column to use for categorization
categorize_by_column = 'A'

# Specify the category for which you want to calculate the means
category_to_calculate = 'test'

# Initialize a dictionary to store means for each index
means_by_index = {}

# Iterate through dataframes and categorize
for df_name, df in dataframes.items():
    category = df[categorize_by_column].iloc[0]
    category_lower = category.lower()  # Convert category to lowercase
    if category_lower not in categorized_dataframes:
        categorized_dataframes[category_lower] = []
    categorized_dataframes[category_lower].append(df)

# Calculate the mean for each available index within the specified category
if category_to_calculate in categorized_dataframes:
    category_dataframes = categorized_dataframes[category_to_calculate]
    
    # Get the unique date indices from the dataframes
    date_indices = set()
    for df in category_dataframes:
        date_indices.update(df.index)
    
    # Calculate means for each unique date index
    for date_index in date_indices:
        values = [df.loc[date_index, 'Value'] for df in category_dataframes if date_index in df.index]
        mean_value = sum(values) / len(values)
        means_by_index[date_index] = mean_value

# Print the list of means for each available date index
for date_index, mean in means_by_index.items():
    print(f"Mean value for date index '{date_index}': {mean}")
==============/////////////=================//////////////
import pandas as pd

# Sample dictionary of dataframes with unique constant values in column 'A'
dataframes = {
    'df1': pd.DataFrame({'A': ['Test'], 'Value': [10]}),
    'df2': pd.DataFrame({'A': ['Kurwa'], 'Value': [20]}),
    'df3': pd.DataFrame({'A': ['Test'], 'Value': [30]})
}

# Create a dictionary to categorize dataframes
categorized_dataframes = {}

# Specify the column to use for categorization
categorize_by_column = 'A'

# Iterate through dataframes and categorize
for df_name, df in dataframes.items():
    category = df[categorize_by_column].iloc[0]
    category_lower = category.lower()  # Convert category to lowercase
    if category_lower not in categorized_dataframes:
        categorized_dataframes[category_lower] = {}
    categorized_dataframes[category_lower][df_name] = df

# Print categorized dataframes
for category, df_dict in categorized_dataframes.items():
    print(f'Category: {category}')
    for df_name, df in df_dict.items():
        print(f'{df_name}:\n{df}\n')





# Sample dictionary with keys containing "-Cstats" suffix
my_dict = {
    'key1-Cstats': 10,
    'key2-Cstats': 20,
    'key3-Cstats': 30
}

# Remove the "-Cstats" suffix from keys
updated_dict = {key.replace('-Cstats', ''): value for key, value in my_dict.items()}

print(updated_dict)



import pandas as pd

def find_first_keys_in_second_dict(my_dict, **key_lists):
    results = {}
    for identifier, value_list in my_dict.items():
        second_dict = value_list[1] if len(value_list) > 1 else {}
        for key_list_name, keys in key_lists.items():
            for key in keys:
                if key in second_dict:
                    if identifier in results:
                        results[identifier][key_list_name] = second_dict[key]
                    else:
                        results[identifier] = {key_list_name: second_dict[key]}
                    break  # Stop searching in this list of keys if a match is found
    return results

# Example usage:
my_dict = {
    "ID1": [{}, {"cherry": 2, "apple": 3}],
    "ID2": [{}, {"banana": 5, "date": 1}],
    "ID3": [{}, {"mango": 8, "lemon": 6}],
}

keys_list1 = ["cherry", "grape", "apple"]
keys_list2 = ["banana", "kiwi", "date"]
keys_list3 = ["mango", "lemon", "apple"]

results = find_first_keys_in_second_dict(my_dict, key_list1=keys_list1, key_list2=keys_list2, key_list3=keys_list3)

if results:
    df = pd.DataFrame(results).T.fillna('Not Found')
    print(df)
else:
    print("None of the keys were found in the dictionaries.")



/////////////###################



# Sample dictionary with multiple keys
data_dict = {
    'DATE_MM': '2023-11-01|2023-11-02|2023-11-03|2023-11-04|2023-11-05|2023-11-06|2023-11-07|2023-11-08|2023-11-09|2023-11-10',
    'CPR': 'CPR1|CPR2||CPR4|NA||||CPR8||',
    'CDR': 'CDR1||CDR3|CDR4|CDR5|||||CDR9|CDR10',
    'Severity': 'Low|Medium|High|Low|Medium|||||High|Low'
}

def process_keys_to_mapping(data, keys):
    mapping = {}
    dates = data['DATE_MM'].split('|')
    for date in dates:
        values = {}
        for key in keys:
            key_values = data[key].split('|')
            value = key_values[dates.index(date)]
            if value and value != "NA":
                values[key] = value
        if values:
            mapping[date] = values
    return mapping

keys_to_process = ['CPR', 'CDR', 'Severity']
date_values_mapping = process_keys_to_mapping(data_dict, keys_to_process)

# Printing the resulting mapping
for date, values in date_values_mapping.items():
    print(f"Date: {date}, Values: {values}")













# Sample dictionary
data_dict = {
    'DATE_MM': '2023-11-01|2023-11-02|2023-11-03|2023-11-04|2023-11-05|2023-11-06|2023-11-07|2023-11-08|2023-11-09|2023-11-10',
    'CPR': 'CPR1|CPR2||CPR4|NA||||CPR8||'
}

# Split the values by '|' to get lists of dates and CPR values
date_mm_values = data_dict['DATE_MM'].split('|')
cpr_values = data_dict['CPR'].split('|')

# Create a mapping between dates and CPR values, skipping empty and "NA" values
date_cpr_mapping = {}
for date, cpr in zip(date_mm_values, cpr_values):
    if cpr and cpr != "NA":
        date_cpr_mapping[date] = cpr

# Example: Get CPR value for a specific date
desired_date = '2023-11-04'
if desired_date in date_cpr_mapping:
    cpr_value = date_cpr_mapping[desired_date]
    print(f"For date {desired_date}, the CPR value is {cpr_value}")
else:
    print(f"No CPR value found for date {desired_date}")




counts_df.index = counts_df.index.astype(str).str.replace(r'[\[\]()]', '').str.replace(', ', '-').str.split(',').str[0]
import pandas as pd
import numpy as np

# Sample data (replace this with your actual data)
data = {
    'column1': np.random.randint(1, 101, 100),  # Generating random data for column1
    'column2': np.random.randint(1, 101, 100)  # Generating random data for column2
}

df = pd.DataFrame(data)

# Combine data from both columns
combined_data = pd.concat([df['column1'], df['column2']])

# Specify the number of bins
num_bins = 10

# Calculate bin edges based on the combined data
min_val = combined_data.min()
max_val = combined_data.max()
bin_width = (max_val - min_val) / num_bins
bin_edges = [min_val + i * bin_width for i in range(num_bins + 1)]

# Apply pd.cut to both columns using the same bin edges
df['bin_range_column1'] = pd.cut(df['column1'], bins=bin_edges)
df['bin_range_column2'] = pd.cut(df['column2'], bins=bin_edges)

# Display the DataFrame with assigned ranges for both columns
print(df)

count_column1 = df['bin_range_column1'].value_counts()
count_column2 = df['bin_range_column2'].value_counts()
counts_df = pd.DataFrame({'Count_Column1': count_column1, 'Count_Column2': count_column2}).fillna(0)
